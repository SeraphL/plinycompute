% http://www.vldb.org/pvldb/vol9/p1707-pirk.pdf
% https://cs.stanford.edu/~matei/papers/2017/cidr_weld.pdf
% http://www.vldb.org/pvldb/vol7/p853-klonatos.pdf

\begin{center}\textbf{PlinyCompute: A Platform for High-Performance, Distributed, Data-Intensive Computing}
\end{center}


\section{Introduction}

Big Data systems such as Spark \cite{zaharia2010spark} and Flink \cite{alexandrov2014stratosphere, carbone2015apache}
have effectively solved what we affectionately call the ``data munging'' problem.  That is, 
these systems do an excellent job supporting the rapid
and robust development of problem-specific,
distributed/parallel codes that transform a raw data set into structured 
or semi-structured form, and then
extract actionable information from the transformed data.
But while existing Big Data systems offer
excellent support for data munging,
there is a class of application for which 
existing systems are 
are
used, but arguably are far less suitable:
as a platform 
for the development of high-performance codes, especially reusable
Big Data tools and libraries, by a capable
systems programmer.

The desire to build new tools
on top of existing Big Data systems is understandable.  
The  developer of a distributed data processing tool must worry about data persistence, movement of
data to/from secondary storage, data
and task distribution, resource allocation, load balancing, fault tolerance, and many other factors.
While classical high-performance computing
tools such as MPI \cite{gropp1996high} do not provide support for all of these concerns,
existing Big Data systems 
address them quite well.
As a result, many tools and libraries have been built on top of existing systems.  For example,
Spark supports
machine learning libraries Mahout \cite{mahout}, Dl4j \cite{dj4j}, 
and MLLib \cite{meng2016mllib}, linear algebra packages such as SystemML \cite{tian2012scalable, boehm2016systemml, ghoting2011systemml, boehm2014hybrid} and Samsara \cite{samsara}, and graph analytics with
GraphX \cite{gonzalez2014graphx} and GraphFrames \cite{dave2016graphframes}.  Examples abound.

\vspace{5 pt}
\noindent
\textbf{PlinyCompute: A platform for high-performance, Big Data computing.}
However, we assert that if one were to develop a system purely for developing high-performance
Big Data codes
by a capable systems programmer,
it would not look like existing systems such as Spark and Flink,
which have largely 
been built using high-level programming languages and managed runtimes such as the Java 
virtual machine.  Virtual machines abstract away
most details regarding memory management
from the system designer, including memory deallocation, reuse, and movement, as well as pointers,
object serialization and deserialization.
Since managing and utilizing memory are 
perhaps the most important factors determining system performance, reliance
on a managed environment can mean an order-of-magnitude increase in CPU cost for some computations.  
This cost may be acceptable if the person using the system
is a programmer uncomfortable with the basics of memory management who is
building an application to complete a specific data munging task.  
But it is unacceptable for high-performance tool or library development by an expert.

This proposal is concerned with the design and implementation of
\emph{PlinyCompute}, a system for development of
high-performance, data-intensive, distributed computing codes, especially tools and libraries.
PlinyCompute, or PC for short, is designed to fill the gap between 
HPC softwares such as OpenMP \cite{dagum1998openmp} and MPI \cite{gropp1996high}, which provide little direct support for
managing very large data sets, and dataflow platforms such as Spark and Flink, which 
may give up significant performance through their reliance on a managed runtime to handle
memory management (including layout and de/allocation) and key computational considerations
such as virtual method/function dispatch to the JVM. 

\vspace{5 pt}
\noindent
\textbf{Core design principle: Declarative in the large, high-performance in the small.}
PC is unique in that \emph{in the large}, 
it presents the programmer with a very high-level,
declarative interface, relying on automatic, 
relational-database style optimization \cite{chaudhuri1998overview} to figure out how to stage
distributed computations.  
PC's declarative interface is higher-level than competing systems, in that decisions such as choice of join ordering and which
join algorithms to run are
totally under control of the system. 
This is particularly important for tool and library development because the same tool should run well regardless of the data
it is applied to---the classical ideal of \emph{data independence} in database system design \cite{stonebraker1990third}.
A relatively naive library user cannot be expected to tune a library implementation of an algorithm to run
well an his or her particular data set, and yet with existing systems, this sort of tuning
is absolutely necessary.  For example, we find
that a high quality LDA implementation\footnote{LDA \cite{blei2003latent} is a popular text mining algorithm.}
on top of Spark is around $25\times$ slower than the algorithmically equivalent LDA
implementation on top of PC.  Through careful, dataset-specific tuning (including choosing specific join algorithms and
forcing pipelining of certain results) it is possible to get that gap down to $2.5\times$.  But this requires modification of the
code itself, which is beyond the vast majority of end-users.

In contrast, \emph{in the small}, PlinyCompute presents a capable programmer with a
persistent object data model and API (the ``Object model'') and associated memory management system
designed from the ground-up for
high performance.
All data processed by PC are managed by
the PC Object
model, which is exclusively responsible for PC data layout and within-page memory management.  
The PC Object model is tightly coupled with
PC's execution engine, and has been specifically designed for efficient distributed computing.  
All dynamic PC Object allocation is \emph{in-place}, directly on a page, obviating
the need for PC Object serialization and deserialization before data are transferred to to/from storage or over a network.
Further, PC gives a programmer fine-grained control of the systems memory management and PC Object re-use policies.

This hybrid approach---declarative and yet trusting the programmer
to utilize PC's Object model effectively
in the small---results in a system that is ideal for the 
development of data-oriented tools and libraries.

\vspace{5 pt}
\noindent
\textbf{Current status and proposed research program.}
Currently, PC exists as a prototype system, consisting of around
150,000 lines of C++ code, with a much smaller amount of Prolog code.
At the time of this proposal's submission, we are preparing or first
research paper describing the PC prototype.

The PC prototype
is able to run complex distributed computations, and in this proposal we will
give strong evidence that the ``declarative in the large, high-performance in the small''
ideal underlying PC can facilitate high-performance and self-tuning large-scale data processing implementations.
But while we have a functional prototype, there is much more work to be done.
Continued engineering effort (such as improving PC's networking code) is important,
and this project will address such concerns.
But there is fundamental research to be done, 
centered around the question:

\vspace{5pt}
\noindent
\emph{What are the design principles that follow from ``declarative
in the large, high-performance in the small,'' and can those principles be used
to design and implement
a high-performance, Big Data system that bridges the gap between HPC and Big Data systems?}

\vspace{5pt}
\noindent
All of the ideas we examine will be implemented and evaluated by implementing them on top of
PlinyCompute.
Specifically, the proposal is concerned with the following five research problems:

\vspace{-7 pt}
\begin{itemize}
\item Optimization of the very large computational plans that follow from recursive computations.

\vspace{-7 pt}

\item Statistical models for combined execution and optimization of sometimes opaque user code.

\vspace{-7 pt}

\item Developing support for cross-block references---``graph-based'' computing.

\vspace{-7 pt}

\item Continuous computations and relaxed models for consistency, specifically for machine learning.

\vspace{-7 pt}

\item Developing PC-specific compiler optimizations utilizing knowledge of PC-specific semantics.
\end{itemize}

\vspace{-7 pt}
In addition, we will continue to develop benchmarks and applications
to evaluate PlinyCompute.  Specifically, we
will continue to develop our prototype \texttt{LilLinAlg} language, and will continue our port of
TensorFlow's automated differentiation API \cite{abadi2016tensorflow} onto \texttt{LilLinAlg} and PlinyCompute.

\section{Broader Impacts and Intellectual Merit}

\vspace{5pt}
\noindent
\textbf{Broader impacts.}
Our preliminary results show 
a computation on top of 
PlinyCompute can be $50+\times$ faster than an algorithmically equivalent computation on top of Spark.
A more typical gap may be $3\times$ to $5\times$.
In the era of cloud computing, a $3\times$ improvement in running time
translates to a $3\times$ reduction in 
cost, as well as a $3\times$ reduction in energy usage.
Given the
large number of widely-used libraries and tools implemented on top of Spark and related systems, 
it will not take long for the 
accumulated reductions in cost, time, and
energy usage to become significant.
Further, PC can increase the scope of computations that are practical
on top of a Big Data system.
Potential developers of high-performance, data-oriented
computations  eschew Spark and its cousins because they run in managed
runtimes.  The obvious alternative is MPI---and yet MPI requires too much development effort for Big
Data computations.  PC can fill this gap.

In addition to the technical impacts, the proposed project will include an outreach to high school 
students.  The project will provide opportunities for students to spend time at Rice and engage with
team members, engaged in computer science research.
All developed software will be open sourced.  

\vspace{5pt}
\noindent
\textbf{Intellectual merit.}
There have a very large number of Big Data 
systems proposed over the last few years \cite{white2009hadoop, palkar2017weld, zaharia2010spark, crotty2015tupleware, yu2008dryadlinq, alexandrov2014stratosphere, low2014graphlab, murray2013naiad, borkar2011hyracks}. The
vast majority make use of managed runtimes, and those systems that do not (such as relational-style systems \cite{bittorf2015impala}) tend
to have limited programming models.  

It is no secret that managed runtimes incur significant performance cost.  Most system designers are content to pay, in return for benefits such as easy movement of executable code and support for heterogeneity.  But code can be moved transparently
in a non-managed runtime (see PC's \texttt{vtable} fixing in Section 5.5) and modern virtualization tools such as Docker \cite{merkel2014docker} 
can alleviate some problems with heterogeneity.    

Some research efforts 
have been aimed at alleviating the costs associated with a managed runtime---for example, by designing/optimizing
garbage collectors for Big Data systems \cite{nguyen2016yak, bruno2017ng2c, maas2015trash, gog2015broom}, improving
object 
de/serialization \cite{horvath2017code, miller2013instant}, or moving toward more restrictive non-object programming models (such as 
Spark's Datasets \cite{datasets} and DataFrames \cite{armbrust2015spark}).

In contrast, PlinyCompute is unique in its emphasis on considering the systems aspects of
how objects should be allocated, managed, and manipulated when \emph{not constrained by a managed runtime}.
And while PC gives a programmer the tools and low-level control to write very fast code,
in the large, PC codes are much higher-level than other systems, resembling classical relational
calculus \cite{codd1971data}.  For example, to specify a join over five sets, 
a, the programmer simply provides the join predicate, and not a join order or algorithm.
Declarativity means that PC can adapt to the data set and hardware, by automatically choosing the
specific algorithms to run and details of the physical implementation.



