
\section{Experiments}

In this section, we describe our experimental evaluation of the
performance of the PC distributed computation framework for a set of
representative big data analytics problems. The aim is to answer
following questions:

\begin {enumerate}
\item How useful is PC in building tools and libraries for big data analytics?
\item How efficient is PC in manipulating highly nested and complex objects?
\item How well is PC in performing complex computations like real
  machine learning applications?
\end {enumerate}

\subsection {Experimental Environment and Applications}

All of the experiments reported in this paper were performed in a
cluster that consists of eleven Amazon EC2 m2.4xlarge machines,
running Ubuntu 16.04. Each machine had eight virtual cores, one SSD
disk, and 68 GB of RAM). One of the eleven machines served as the Master
node and the rest ten machines served as Worker nodes.

\vspace{5pt}
To conduct convincing benchmarks and performance comparisons, we
selected eight representative applications from three categories
of big data analytics workloads that are important for demonstrating
PC's utilities and performance:

\begin {itemize}
\item \texttt{Linear Algebra Library.} We constructed a linear algebra library
  and tested three common linear algebra computations: Gram Matrix,
  Linear Regression, and Nearest Neighbor Search.
\item \texttt{Analytical queries on TPC-H Data
    Set.}  We implemented two typical analytical queries against TPC-H
  benchmark data set that were denomalized to complex nested objects: the first one is an aggregation query reporting which customer buys which
  parts for each supplier; the second one is a Top-K similarity query
  to search for K customers who buy the closest items compared with a
  given customer.
\item \texttt{Machine Learning.} We also implemented three widely used
  iterative and complex machine learning algorithms: \textbf{Latent Dirichlet Allocation (LDA)}--A
  generative statistic model for text topic mining;
  \textbf{Gaussian Mixture Model (GMM)}--A clustering algorithm to generate a composite
  distribution whereby points are drawn from one of multiple Gaussian distributions;  and \textbf{KMeans}-- A well-known clustering algorithm that clusters
  data points into a pre-defined number of clusters.
\end {itemize}

\subsection {Construction of a linear algebra library}
Since PC is designed to support the construction
of high-performance tools and libraries, our first benchmarking effort was aimed at determining 
whether PC is actually useful for that task.  Thus, we asked
a graduate student who knew nothing of PC to use the system to build a small Matlab-like 
programming language and library for distributed matrix operations.
We called this implementation \texttt{LilLinAlg}.
Our goal was to determine the 
performance and functionality that an expert programmer (but PC novice) could deliver in a short
timeframe, compared to a set of established distributed Big Data matrix implementations:
SciDB \cite{brown2010overview, stonebraker2011architecture} (built from the ground up by an MIT team over the last nine years), MLLib \cite{meng2016mllib} 
(the Big Data matrix
implementation shipped with Spark), and SystemML \cite{boehm2014hybrid, ghoting2011systemml, boehm2016systemml}
(a matrix and machine learning implementation developed
over the last seven years by a team at IBM, built on top of Spark and Hadoop).
The student spent about six weeks in this effort.

We ran three different computations:
a Gram matrix computation (given a matrix $\textbf{X}$, compute
$\textbf{X}^T \textbf{X}$), least squares linear regression (given a matrix of features $\textbf{X}$ and
responses $\textbf{y}$, compute 
$\hat{\pmb{\beta}} = (\textbf{X}^{T} \textbf{X})^{-1} \textbf{X}^{T} \textbf{y}$), and nearest
neighbor search in a Riemannian metric space \cite{lebanon2006metric} encoded by matrix $\textbf{A}$ (that is,
given a query vector
$\textbf{x}'$ and matrix $\textbf{X}$, find the $k$ rows in the matrix that minimize 
$d_{\textbf{A}}^2(\textbf{x}_i, \textbf{x}') = 
(\textbf{x}_i - \textbf{x}')^T\textbf{A}(\textbf{x}_i - \textbf{x}')$).  All experiments used
ten Amazon
EC2 m2.4xlarge workers.  For
all three computations, 
$10^6$ data points were used.  Results are given in 
Table \ref{fig:LR} (for Gram and regression, SystemML V0.9 on Hadoop,
and Spark 1.6.1 was used; for
nearest neighbor, SystemML V1.0 on Spark and Spark 2.1.0 was used).


\begin{table}[h!]
\begin{center}
\begin{tabular}{|c||c|c|c||c|c|c||c|c|c||}
\hline
& \multicolumn{3}{c||}{Gram Matrix} & \multicolumn{3}{c||}{Linear Regression} & \multicolumn{3}{c||}{Nearest Neighbor} \\
\hline
Dimensionality & $10$ & $100$ & $1000$& $10$ & $100$ & $1000$& $10$ & $100$ & $1000$ \\
\hline
\hline
PC (\texttt{LilLinAlg}) &00:07 & 00:09 &00:39 &00:14 &00:22 &00:49& 00:15 & 00:20 & 01:06 \\
SystemML &00:05$*$ &00:51 &02:34 &00:06$*$ &00:53 &02:38 &00:04$*$ &00:30 &01:32 \\
Spark \texttt{mllib} &00:20  &00:54 &17:31 &00:35 &01:01 &17:42 &01:20 & 04:49 &14:30 \\
SciDB   &00:03 &00:17 &03:20 &00:15 &00:33 &06:04 &00:28 &02:56 & 06:24 \\
\hline
\end{tabular}
\caption{Linear algebra benchmark. Format is MM:SS.
A star ($*$) indicates running in local mode.}
\label{fig:LR}
\end{center}
\end{table}

In every case except for the small problems when SystemML chose not to distribute the data,
\texttt{LilLinAlg} was the fastest.  
Though SystemML nearest neighbor approached the speed of 
\texttt{LilLinAlg}, we point out the vast difference in engineering effort between the two systems.  
SystemML was built over many years by a team of PhDs. Research papers have been written about the
technology developed for the system, including one awarded a VLDB best paper award \cite{boehm2016systemml}.
\texttt{LilLinAlg} was developed in six weeks, and it is faster (though to be fair, SystemML has a much broader
set of capabilities than \texttt{LilLinAlg}).

\vspace{5pt}
\noindent
\textbf{Summary.} This set of experiments demonstrated that a
domain-expert who has a few years' C++ programming experiences can fully control and
exploit the bare-metal computing power of PC through PC's highly
declarative and highly tunable interfaces.

\subsection{Big object-oriented data manipulation}
For an example of the raw performance of the PC object model and its ability to power large-scale 
object-oriented computations, we use the TPC-H benchmark data set \cite{council2008tpc}
and de-normalize
the data into a large set of \texttt{Customer} objects. Each
\texttt{Customer} object contains, among
other data, a list
or \texttt{Order} objects, and each \texttt{Order} object contains a list of \texttt{Lineitem} objects,
each of which has a \texttt{Part} and \texttt{Supplier} object.  
We run two computations. First, we compute, for each supplier,
the set of parts that the supplier has sold to each customer (stored
as a map from customer name to a list of part identifiers).
Second, we compute the $k$ customers whose set of \texttt{Part} items purchased is closest to
a query set, according to Jaccard similarity.
Because the data are object-oriented, it
is not possible to use Spark's Dataset/DataFrame abstraction to implement this computation, and so
we compare algorithmically equivalent Spark RDD (using Spark 2.1.0) and PC implementations.
Results are in Table \ref{fig:TPC}, run on the same 10-worker cluster.  

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|c|}
\hline
Kryo data size: &41.5GB & 83.1GB & 167.2GB &251.1GB &333.2 &416.2GB \\
\hline
& \multicolumn{6}{c|}{\texttt{Customer}s per \texttt{Supplier}} \\
\hline
PlinyCompute: hot PDB & 00:11&	00:19&	00:35&	00:51&	01:08&	01:21 \\
Spark: hot HDFS & 01:04&	01:53&	03:24&	04:54&	06:25&	08:16\\
Spark: in-RAM deserialized RDD & 00:16& 	00:29& 	00:56& 	01:21& 	02:18& 	03:56\\
\hline
& \multicolumn{6}{c|}{top-$k$ Jaccard} \\
\hline
PlinyCompute: hot PDB & 00:03&	00:03&	00:04&	00:05&	00:05&	00:06 \\
Spark: hot HDFS & 00:56&	01:38&	03:01 & 04:01&	05:22&	06:34\\
Spark: in-RAM deserialized RDD & 00:08& 	00:12& 	00:21 & 00:32& 	01:11& 	02:38\\
\hline
\end{tabular}
\caption{PlinyCompute vs. Spark for large-scale OO computation. Times in MM:SS.}
\label{fig:TPC}
\end{center}
\end{table}
\vspace{-10pt}
We see that in a somewhat fair comparison, when PC data are
stored in a hot Pliny Database (PDB for short; PDB is PC's backend storage system) and Spark data
are stored in
a hot HDFS, the computation is $6\times$ to $66\times$ faster in PC.  We say ``somewhat'' fair
because at the end of the computation, Spark is left with a large number of objects that still need to be 
garbage collected by the JVM---a cost not included in the computation. If the data are already
fully deserialized and stored as an RDD, PC is 
between 50\% and $26\times$ faster.  However, we point out that this is not an apples-to-apples comparison.
Among other costs, it ignores
the cost Spark would incur at a later time to free the objects stored
in RAM.

\vspace{5pt}
\noindent
\textbf{Summary.} The components of the TPC-H database are defined to
consist of eight separate and individual tables. After translated to
object-oriented data representation, a denormalized object is highly
nested and requires significant amount of dereference
operations to complete a query. Therefore, we regarded this set of
experiments as reasonable stress
tests for manipulating complex objects. 

The experimental results showed that
compared with Java objects, the PC
object model had a significant performance improvement, which were
brought by the native code advantage and PC object model features such
as no serialization/deserialization overhead for moving data. It
also proved that the costs associated with PC object model such as
vtable fixing can be minimized via careful engineering.

\subsection {Machine Learning}
\vspace{5pt}
\noindent
\textbf {(1) LDA.} We first describe our experience writing a word-based, non-collapsed LDA implementation \cite{jermaineExperimental} on top of
both Spark and PC.  LDA is a standard text mining algorithm and
non-collapsed. 
LDA is chosen because
it is challenging:
it contains two joins and two aggregations among other operations.
Our Spark combined RDD and DataSet        
implementation (using Spark 2.1.0) was carefully engineered by an expert.
On the cluster used before, we run LDA over a 2.5 million document
database.  Results (per iteration) are illustrated in Tab.~\ref{fig:LDA}:

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|c|}
\hline
PlinyCompute & \makecell{Spark 1: \\vanilla} & \makecell{Spark 2: also with \\join hint} & \makecell{Spark 3: also with \\forced persist} & \makecell{Spark 3: also hand-\\coded multinomial} \\
\hline
02:05 & 50:20 & 17:30 & 09:26 & 05:26 \\
\hline
\end{tabular}
\caption{PlinyCompute vs. Spark for LDA. Times in MM:SS, averaged over five iterations.}
\label{fig:LDA}
\end{center}
\end{table}
\vspace{-10pt}
While Spark performed well, the 
amount of work required to arrive at a good solution 
was significant, representing about a week of tuning.  First, among other things, our Spark expert had to force a 
broadcast join.  Then, it was necessary to force Spark to
persist the result of one of the joins for later use.  Finally, it was necessary to hand-code a 
Multinomial sampler to obtain an implementation that was competitive with PC.
This illustrates the advantage of PC's declarative approach: decisions such as broadcast vs. full hash
join as well as which intermediate results to materialize are
automated. 

\vspace{5pt}
\noindent
\textbf {(2) GMM.} A Gaussian mixture model (GMM) can be used to model data points that comes
from one of $K$ clusters, and data points within the same cluster
can be modeled by a Gaussian distribution. So the $k$-th Gaussian
distribution is parameterized by a mean vector $\mu_k$ and a covariance
matrix $\sum_k$, and has a probability distribution $\pi_k$.

For this experiment, we
implemented GMM using the Expectation Maximization (EM) algorithm to be
consistent with the GMM algorithm implemented in Spark Mllib. EM is an
iterative optimization technique that consists of two steps in each
iteration: an estimation step to determine ``responsibility'' score
for assigning each data point to each Gaussian $k$ using the current model; and a maximization step to
update the model parameters using the new ``responsibility'' score. A complex
aggregation will run in each iteration to complete the two steps. The algorithm repeats
the two steps until log likelihood or parameters converges. 

We compared PC with Spark MLLib (using Spark 2.1.0) for $10^7$ data
points with 100 dimenisons, and $10^6$ data points with 300 and 500
dimensions. All experiments were run in the 10-worker cluster aforementioned. 
As illustrated in Tab.~\ref{fig:Gmm}, PC achieves a $2\times$ to
$3\times$ speedup compared with Spark MLLib. For Spark MLLib, we
carefully tuned the data partition size to make sure there are enough
partitions to keep all CPU cores busy.

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c||c|c|c||}
\hline
Dimensionality & $100$ & $300$ & $500$ \\
Number of points & $10^7$ & $10^6$ & $10^6$ \\
\hline
\hline
PlinyCompute &00:33 & 00:53 & 2:16 \\
Spark \texttt{mllib} &1:41  &1:54 &5:05 \\
\hline
\end{tabular}
\caption{PlinyCompute vs. Spark for GMM. Times in MM:SS, averaged over five iterations.}
\label{fig:Gmm}
\end{center}
\end{table}
\vspace{-10pt}


\vspace{5pt}
\noindent
\textbf {(3) KMeans.} We developed a KMeans implementation on PC, which
is equivalent with the KMeans implementation in Spark MLLib. Spark
MLLib KMeans implementation has two initialization approach: KMeans++
and random initialization using sampling. For both experiments, we use
random initialization by sampling $K$ data points as initial
cluster centroids. For initialization, both implementations also compute $L^2$ norm for each data
point, and transform each data point into a new form with $L^2$ norm value
associated. 

Then in each iteration, each data point finds the closest cluster
of which the centroid has the closest distance with that data point, and assigns the closest
cluster label to itself. When computing the distance of a data point to a
centroid, a lower bound based on $\|a - b\| \geq |\|a\| - \|b\||$ is
first computed to avoid unnecessary distance computation. Then an aggregation is run to update centroids
based on computed new labels. Iteration repeats until centroids
converge.


We compared PC with Spark MLLib (using Spark 2.1.0) for $10^9$ data
points with 10 dimenisons, $10^8$ data points with 100 dimensions,  and $10^7$
data points with 1000
dimensions. All experiments were run in the 10-worker cluster aforementioned. 
As illustrated in Tab.~\ref{fig:KMeans}, PC achieved a $2\times$ to
$4\times$ speedup compared with Spark MLLib. 

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c||c|c|c||c|c|c||}
\hline
& \multicolumn{3}{c||}{Initialization Latency} & \multicolumn{3}{c||}{Average
                                         Iteration Latency} \\
\hline
Dimensionality & $10$ & $100$ & $1000$ & $10$ & $100$ & $1000$\\
Number of points & $10^9$ & $10^8$ & $10^7$ & $10^9$ & $10^8$ & $10^7$\\
\hline
PlinyCompute &3:59 & 1:12 & 00:57 &00:37 & 00:09 & 00:06\\
Spark \texttt{mllib} &9:06  &4:18 &3:20 &01:02 & 00:28 & 00:23\\
\hline
\end{tabular}
\caption{PlinyCompute vs. Spark for KMeans. Times in MM:SS, averaged over five iterations.}
\label{fig:KMeans}
\end{center}
\end{table}


\vspace{5pt}
\noindent
\textbf{Summary.} For iterative and complex machine learning
benchmarks, compared with equivalent and carefully tuned implementations on the latest
Spark platform, PC still achieved $2 \times$ to $4 \times$
speedup. On one side this demonstrated PC's utility in handling
complex and iterative computations. On the other side, we've found that due to the complexity in mathematical and statistical
computations in those workloads, in many cases performance comparison is reduced to comparisons of
language efficiency and compiler supports. So it also indicates that PC has a
potential to achieve even larger performance gain by better
synergizing with C++ compiler and link-time optimizing tool chains.







