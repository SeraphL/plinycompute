
\section{Related Work}

The PC project was inspired by works from database engines, distributed in-memory processing
frameworks,  programming languages and compilers, which can be
categorized into two classes for convenience of discussions: systems and techniques with managed
run-times, and without managed run-times.


\subsection {Systems and Optimizations for Managed Run-times}

Most recent big data query processing and analytics
systems are implemented using
high-level programming languages such as Java and Scala  ~\cite{dean2008mapreduce, yu2008dryadlinq,
  neumann2011efficiently, zaharia2012resilient,
  alexandrov2014stratosphere, klonatos2014building,
  crotty2015tupleware, armbrust2015spark} which rely on a managed
runtime such as the JVM for object (de-)allocation and memory and virtual function call management. Numerous papers
have looked at the problem of mitigating the costs incurred
works propose techniques aiming at
reducing the overhead incurred by the managed runtime.  Ideas examined include
code generation, garbage collector (GC) tuning, and the
use of off-heap memory and structured objects.
We briefly discuss some of those efforts now.

\vspace{5pt}
\noindent
\textbf{Code Generation.} The basic idea is that a system implemented in high level language
can generate native query execution code that manually manages
memory and also avoids virtual function call
overheads. DryadLinq~\cite{yu2008dryadlinq} allows a user to express
distributed data flow
computations in a high-level language like C\# and strongly typed .NET
objects, and it compiles those computations into .NET assembler.
the cluster. 
% Chris note: comparing with a relational system seems out of place
% Hyper~\cite{neumann2011efficiently} is a relational system,
% and it proposes a code
% generation strategy for query compilation to translate relational
% algebra to LLVM assembler for execution.  
LegoBase~\cite{klonatos2014building} switches the interface
from declarative SQL to a high-level language (Scala) and uses a query engine
written in Scala as a code generator to emit specialized and low-level
C code for execution. TupleWare~\cite{crotty2015tupleware} supports
multiple high-level languages (any language with an LLVM compiler) 
and aims to
optimize for UDFs by utilizing code
generation to integrate UDF code with the engine 
code. 
Weld~\cite{palkar2017weld} is a recent system developed in Scala and
Python. It proposes
a common runtime for data analytics libraries by asking library
developers to express their work using a new intermediate
representation (IR) and compiles this IR into multi-threaded code using
LLVM.  Then, application developers can use unified APIs to
call different libraries from Weld. Since version 2.0, Spark~\cite{zaharia2012resilient}
also exploits whole-stage code generation to generate JVM 
code.  The goal is mainly to reduce type parsing and virtual function call
overhead. PC uses a form of code generation (template metaprogramming) but 
the emphasis is quite different, in the sense that the goal is to allow for
efficient distributed programming with complex objects.

\vspace{5pt}
\noindent
\textbf{Optimized Memory Management.} 
Apache Flink~\cite{alexandrov2014stratosphere} and Apache
Spark~\cite{zaharia2012resilient} are both distributed in-memory
dataflow systems. They both provide high level language interfaces (Java and Scala).
Spark SQL~\cite{armbrust2015spark} is a relational system built on top
of Spark. These systems all attempt to 
alleviate garbage collection overhead by storing data
in untyped byte arrays or even storing data off-heap.

Apache Flink~\cite{alexandrov2014stratosphere} in particular
assigns memory budgets to its data processing operators. Upon
initialization, a computation requests a memory budget from the
memory manager and receives a corresponding set of byte arrays as memory segments. 
Each operation will then have its own memory pool that it can manually
manage. 
Apache SparkSQL~\cite{armbrust2015spark} serializes 
relational table into byte arrays and stores the serialized bytes
in a main-memory columnar storage. Spark Tungsten~\cite{tungsten}
optimizes the Spark execution backend by grouping execution
data (such as hashed aggregation data) 
into byte arrays and data can be allocated off-heap via
the sun.misc.Unsafe API, reducing
GC overhead. Deca~\cite{lu2016lifetime} is a memory management framework aiming at
reducing GC overhead. It stores
various Spark data types, e.g. UDF variables, user data and
shuffle data into different
off-heap containers so that objects in each container can have a similar
lifetime and can be recycled together.
All of these methods attempt to alleviate GC overhead; in contrast, PC simply does
not use a managed runtime.

\vspace{5pt}
\noindent
\textbf{Relational Processing on Binary or Structured Objects.} The idea here is to convert or
serialize a Java Object into an efficient binary representation and directly
operate on this binary representation, thus reducing serialization and
deserialization overhead.  This 
binary data representation may also be more space efficient. In addition, binary objects
with similar lifetimes can be easily stored in consecutive byte
arrays, significantly reducing GC overhead.

Apache Flink~\cite{alexandrov2014stratosphere} uses reflection
to analyze Java/Scala object types, and it
maps each object type to one of a limited set of
fundamental data types, such as Java primitive types, an array type,
a Hadoop Writable type, a Flink fixed-length tuple, and so on. Each
fundamental data type is associated with a serializer.  Certain data
types provide comparators to efficiently compare binary
representations and extract fixed-length binary key prefixes without
deserializing the whole object.

Spark~\cite{tungsten} has introduced the Dataset/Dataframe representations
to complement the more OO 
RDD representation~\cite{zaharia2012resilient}. Datasets/Dataframes are
binary data representations used to encode JVM objects relationally (as a
row with various fields/columns). 
Datasets/Dataframes enable relational-style processing
through a relational query optimizer called Catalyst and
also enables Java intermediate code generation to reduce virtual
function call overhead through Tungsten~\cite{tungsten}. 

Such techniques significantly boost performance, by moving away from a flexible, object-oriented
type of system to a more relational system.
It is known that relational systems can be fast, but they limit the sort of applications that
can easily be coded on top of the system.  In contrast, PC attempts to offer a fully 
object-oriented interface.

\subsection {Related Native Systems}
Despite the decreasing performance gap between Java/Scala and C/C++,
operating systems and many tools or libraries are still developed in C/C++. For example,
all of the popular numerical processing libraries utilized in our benchmark are
implemented in C/C++. 
While it is rarer for modern Big Data systems to be implemented natively, some systems do exist.
We now categorize and describe such systems.

\vspace{5pt} 
\noindent
\textbf{HPC Systems.} HPC systems such as Charm++~\cite{kale1993charm++}, OpenMP~\cite{dagum1998openmp}, Cilk~\cite{blumofe1996cilk},  Intel's Array Building
Blocks~\cite{newburn2011intel}, and Threading Building Blocks~\cite{reinders2007intel} are built on
low-level interfaces such as MPI~\cite{gropp1996high}.
Systems built using these tools can be very fast.  However, as described in the introduction to the paper, they are not often used
for modern Big Data programming.

\vspace{5pt} 
\noindent
\textbf{Query Processing.} Many of the ideas underlying PC's vectorized processing engine
were pioneered in various relational systems.  Vertica/C-Store~\cite{stonebraker2005c} proposes
to store and process relational tables by
column to optimize for read performance.
VectorWise~\cite{zukowski2012vectorwise}, which grew out of the MonetDB/X100 project, makes use of
vectorized query execution to amortize the virtual function call overhead across records---just like PC---and also
to exploit SIMD support
on modern hardware. Voodoo~\cite{pirk2016voodoo} uses a new
declarative algebra as the compilation target for query plans, to
enable tuning for hardware specifics like caches, SIMD registers,
GPU and so on. It requires a relational front-end, and serves as an alternative backend for
relational databases such as MonetDB and Hyper. While PC TCAP compilation
and processing is closely related to prior ideas from work in 
code generation and vectorized processing (and PC could, for example,
leverage Voodoo's hardware tuning ideas), PC is non-relational, and attempts to
facilitate high-performance processing on top of a 
highly expressive object model.

\vspace{5pt} 
\noindent
\textbf{Cloud Frameworks.} Impala~\cite{bittorf2015impala} is a
C++-based 
SQL query engine that relies on Hadoop for scalability and 
flexibility in interface and schema. Impala compiles SQL 
into LLVM assembler.
However,
Impala uses a relational data model (though it can read/write
semi-structured data in storage formats such as Arvo, Parquet, RC and so
on from/to external storage such HDFS, using standard
serialization/deserialization methods).

Spanner~\cite{bacon2017spanner} is a
distributed data management system that backs various operational services
at Google. Spanner started out as a key-value store and evolved into a
relational database system with a SQL query processor. It implements a
dialect of SQL, called \emph{standard SQL}, which uses arrays and structures to
support nested data as a first class citizen. To integrate
with user applications, the relational data described by standard SQL needs to
be translated to protocol buffers or user languages through the
GoogleSQL library that contains a compiler front-end and a library of
scalar functions. PC takes a fundamentally different approach, as all code is 
object oriented rather an a mix of SQL and other high (or medium) level languages.

Tensorflow~\cite{abadi2016tensorflow} is a
distributed computing framework mainly designed for deep learning. It
mainly supports processing of numerical data with a
very limited set of types.
Tensorflow provides
a much lower level API than PC's declarative interface, based on tensors,
variables and sessions.
declarative interface. 


