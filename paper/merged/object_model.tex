\section{Details of the Object Model} 
\label{sec:ObjectModel}

At the core of the system is the PC object model, which allows 
programmers to create, manipulate, and store persistent objects.
In keeping with our vision of granting programmers fine-grained control over how data are managed in the small, the PC object model
is much lower level than what is found in systems targeted more towards application programming, yet still provides a great deal of
key functionalities.

\subsection{PC Objects}

Arguably, the choice of how individual data items are to be represented and manipulated
in a data analytics or management system is one of the most
%consequential%
controversial decisions
that a system designer can make, both in terms of 
the programmability of the resulting system, and its performance.  For decades, the dominant model used in
data management was the flat relational model, which 
can achieve very good performance.
Flatness generally means 
that there is typically no distinction between the in-memory representation of data, and the on-disk (or in-network) representation of
data. Thus there is no (de-)serialization cost to move data to/from
disk and network, and memory management costs are very low. 

The downside is that flat relations are very limiting to a programmer.  Modern, object-based 
data analytics systems 
(such as Spark via its Resillient Distributed Dataset (RDD) interface \cite{zaharia2012resilient}) offer far more flexibility, at the (possible) cost of significant performance degradation.  
PC attempts to combine this flexibility with excellent performance.
The PC object model provides a fully object-oriented interface, supporting the standard functionality expected in a modern, object-oriented type system,
including generic programming (the PC object model supports generic Map and Vector types), pointers (or, more specifically,
``pointer-like'' objects called PC \texttt{Handle} objects), inheritance, and dynamic dispatch for runtime polymorphism.  

For example, imagine that the goal is implementing a distributed linear algebra system on top of the PC object model, where huge matrices are ``chunked'' into
smaller sub-matrices.  A sub-matrix may be stored via our current, C++ binding, using the following object:

\begin{code}
class MatrixBlock : public Object {
public:
	int chunkRow, chunkColumn;
	int chunkWidth, chunkHeight;
	Vector <double> values; 
};
\end{code}

or, a sparse sub-matrix may be stored as:

\begin{code}
class SparseMatrixBlock : public Object {
public:
	int chunkRow, chunkColumn;
	int chunkWidth, chunkHeight;
	Map <pair <int, int>, double> values; 
};
\end{code}

In the sparse sub-matrix, the \texttt{pair <int, int>} indexes a non-zero entry in the chunk by its row and column.

But while the PC object model provides a rich, object-oriented programming model, it also provides the good performance characteristic
of a flat relational model.
The key principle underlying the PC object model is \emph{zero-cost data movement}.  That is, once a data object
has been allocated and populated, moving the object to disk or across the network should be a simple matter of copying memory; there
should be no CPU cost for serialization and deserialization.

At first glance, it would seem to be impossible to offer zero-cost data movement while allowing a programmer to create and manipulate such objects.  
Pointers and container classes 
generally lead to high memory (de)allocation costs and high object (de)serialization costs, resulting in high CPU cost.
The PC object model avoids this 
by using a ``page-as-a-heap'' memory allocation model.  
The PC object model provides a call of the form:

\begin{code}
makeObjectAllocatorBlock (ptr, blockSize);
\end{code}

After such a call, \emph{all subsequent PC Object allocations by the thread creating the object allocation block will be performed directly to the memory
region starting at location} \texttt{ptr}.
Typically, when it runs a computation, PC's execution engine will obtain a page from its buffer pool to buffer output data, calling
PC's
\texttt{makeObjectAllocatorBlock ()} function with a pointer to the page where output data are to be written.  
When an action taken by the execution engine or user-supplied code causes an
out-of-memory execution, it means that the page is full.  At that point, the execution engine can take appropriate computation-specific 
action, such as creating 
an object allocation block out of a new (empty) page, writing the full page out to disk, sending it across the network, etc.  
No serialization or deserialization or any sort of post-processing of the page are needed, 
because all object allocations have taken place exclusively to the current allocation block.  

In order to guarantee zero-cost data movement,
one rule that a PC programmer must follow is that any object that will
be loaded into a distributed PC cluster must either be of a ``simple'' type (a simple type
must contain no raw C-style pointers and no 
virtual functions, and a \texttt{memmove}
must suffice to copy the object), or else it must descend from PC's \texttt{Object} class, which serves as the base for all complex object types.
Complex objects are those that include containers (\texttt{Vector}, \texttt{Map}) or pointer-like \texttt{Handle} objects.  Descending from PC's \texttt{Object} class
ensures that the resulting class type has a set of virtual functions
that allow it to be manipulated in and transferred across the
distributed PC cluster, 
such as a virtual deep copy function.  

\subsection{PC Handles}

To support linked data structures, dynamic allocation, and runtime polymorphism, it is necessary for a 
system to provide pointer-like functionality.  This is provided by PC's built-in \texttt{Handle} type.  
A \texttt{Handle} to an object is returned from a dynamic allocation to the current allocation block.
For example, a PC programmer 
can issue the statement:

\begin{code}
Handle <MatrixBlock> mySubMatrix = makeObject <MatrixBlock> ();
\end{code}

Internally, PC \texttt{Handle} objects contain two pieces of data: an \emph{offset pointer} that tells how far
the physical address of the object being pointed to is from the physical location of the 
\texttt{Handle}, and a \emph{type code} that stores the type of the
object that is pointed to.  

PC uses an offset pointer rather than a classical, C-style pointer in order to support
zero-cost data movement.  
A \texttt{Handle} may begin its life allocated
to one page, which may be stored on disk, then sent across a network
to another process.  
An actual C-style pointer
cannot survive translation from one process to another, as the program will be mapped to a different location in memory.
In contrast, at the new process, the \texttt{Handle} pointer
can function correctly. As long as the target of an offset pointer is stored in the same page, an offset pointer will be valid if the
page is copied in its entirety, including all \texttt{Handle}s and their targets.

\subsection{Dynamic Dispatch}
\label{sec:dyn_dis}

Supporting dynamic dispatch for virtual function calls is fundamental to the PC object model.
In PC, dynamic dispatch is facilitated by the type code stored within each
\texttt{Handle} object.
Each type code begins with a bit that denotes whether or not the referenced type is a simple type (which, by definition, cannot have any
virtual functions and for which a \texttt{memmove} suffices to perform a copy) or a type descended from PC's \texttt{Object} base class.
In the case of a simple type, the remaining bits encode the size of the referenced object.  

In the case of a PC \texttt{Object} or its descendants, the
type code is a unique identifier for the PC-\texttt{Object}-descended type of the object that the \texttt{Handle} points to.
In every major C++ compiler (GCC, clang, Intel, and Microsoft), virtual functions
are implemented using a virtual function table, or \texttt{vTable} object, a pointer to which is located at the beginning
of each C++ object having a virtual function.  Unfortunately, the \texttt{vTable} pointer is a native, C-style pointer, the
\texttt{vTable} pointer does not automatically translate when an
object is moved from process to process.  To handle this, in
PC's C++ binding, whenever a PC \texttt{Handle} object is dereferenced, 
a lookup on the
type code is performed transparently to the application programmer.  This lookup retrieves a process-specific pointer to that class' \texttt{vTable} object, which is
then placed at the head of the object.

Obtaining a pointer to a class' \texttt{vTable} object is not straightforward.
A user may run code on his/her machine that creates
a PC \texttt{Object}, and then ship that PC \texttt{Object} into the
PC cluster.  At the other end, it arrives at a PC worker process that has never seen that type of object before and hence
does not have access to a \texttt{vTable} pointer for that class.
PC addresses this issue by requiring that all classes deriving from PC's \texttt{Object} base class be registered with the PC catalog
server before they are loaded into the distributed storage subsystem.  This registration requires shipping a library file (a \texttt{.so} file in Linux/Unix) to
the catalog server.  This library exposes a special
\texttt{getVTablePtr ()} function that returns a C-style raw pointer to the \texttt{vTable} for the class contained
in the \texttt{.so} file.

Whenever there is a \texttt{vTable} pointer lookup, the request first goes to the PC process' \texttt{vTable} lookup table.  When this 
lookup fails (because the process has not yet seen a \texttt{vTable} pointer for that class
type) the request then goes to the PC cluster's catalog server, which responds to the process with a copy of the appropriate \texttt{.so} file.  This 
\texttt{.so} file is then 
dynamically loaded into the process' address space, \texttt{getVTablePtr ()} is called, 
and the located \texttt{vTable} pointer is loaded into the lookup table, and then copied into the PC \texttt{Object} that is being referenced.  

In this way, PC provides something akin to the automated,
dynamic loading of classes (via Java Virtual Machine \texttt{.class} files) that is
provided by most big data systems.  
Objects of arbitrary type can be loaded into the distributed PC cluster and be processed using dynamically-loaded native code, as long
as the object type is registered first.

\subsection{Allocation, Deallocation, and Cross-Block Assignment}

There are three types of allocation blocks in PC, where an ``allocation block'' is a block of memory where PC \texttt{Object}s can be
allocated, or where they are located.

\begin{enumerate}

\item Each thread running in a PC process has exactly one \emph{active} allocation block, that is currently receiving allocations (all calls to
\texttt{makeObject} cause memory allocations to happen using that block).  Such an allocation block is created via a call to 
\texttt{makeObjectAllocatorBlock ()}.  User code typically creates and manipulates objects in this block.

\item Each thread also has one or more
\emph{inactive}, \emph{managed} blocks.  These are previously-active blocks of memory that contain one or more objects that are reachable
from some \texttt{Handle} that is currently in RAM.  When the number of reachable objects in an inactive, managed block drops to zero, it is automatically
deallocated.
When a user (or the PC system software) calls 
\texttt{makeObjectAllocatorBl ock ()}, the newly created allocation block becomes the active block, and if the previously-active allocation block has any
reachable objects on it, it becomes an inactive, managed block.

\item Finally, there are zero or more \emph{inactive},
  \emph{un-managed} blocks.  These are blocks with reachable PC
  \texttt{Object}s that are \emph{not} managed by the PC object model.  These tend to be pages of objects that have
been loaded into RAM from disk or across the network for processing
during a distributed computation.  Such blocks are paged in and out of the
buffer pool in much the same way as a relational database would page data in and out.
Rather than the PC object model being responsible for managing such blocks, PC's execution engine manages such blocks.
Further, since managed blocks are only managed by the ``home'' thread where
they are created, a managed block is effectively un-managed when viewed from
any other thread.

\end{enumerate}

In PC, each managed allocation block (active or inactive) has an active object counter (the number of objects that are reachable
from some \texttt{Handle} in RAM).  Each object in each managed allocation block (active or inactive) is reference counted, or pre-pended with a count of
the number of \texttt{Handle} objects that currently reference the object.  
Un-managed blocks (and objects inside of such blocks) are not reference-counted.

When the reference count on an object in a managed block goes to zero, it is automatically
deallocated (at least, this is the default behavior; it is possible for a programmer to override this behavior for speed, if desired, as we describe in Section~\ref{sec:allocation}).  
Once the number of reachable objects on an inactive, managed allocation block falls to zero, the block is automatically deallocated.  
In that sense, PC resembles a smart-pointer based memory management system.  

Since the fundamental goal of PC object model design is 
zero-cost data movement---an allocation block should be transferable across processes and immediately usable with no pre- or post-processing---one
potential problem is dangling \texttt{Handle}s.  Specifically: What happens when there is a \texttt{Handle} located in one allocation block that points to a PC
\texttt{Object} located in another allocation block?  The \texttt{Handle} may be valid, but when the \texttt{Handle}'s allocation block is moved to a new process where
the target block is not located, the \texttt{Handle} cannot be dereferenced without a runtime error. 
PC simply prevents this situation from ever happening. Whenever an assignment operation on \texttt{Handle} that is physically located
in the active allocation block results in that
\texttt{Handle} that is physically located in the active allocation block pointing outside of the block, a deep copy of the target of the assignment
is automatically performed.  This deep copy happens recursively, so any \texttt{Handle}s in the copied object that point outside of the active allocation block
have \emph{their} targets deep-copied to the active block.  For example, consider the following code:

\begin{code}
makeObjectAllocatorBlock (1024 * 1024);
Handle <Vector <double>> data = makeObject <Vector <double>> ();
for (int i = 0; i < 1000; i++)
     data->push_back (i * 1.0);
makeObjectAllocatorBlock (1024 * 1024); 
Handle <MatrixBlock> myMatrix = makeObject <MatrixBlock> ();
myMatrix->value = data; // deep copy of data happens!!
\end{code}

At the second \texttt{makeObjectAllocatorBlock}, the original allocation block, holding the list of \texttt{double}s pointed to by \texttt{data}, becomes
inactive.  The submatrix \texttt{myMatrix} is allocated to the new active block.  
Hence, the assignment of \texttt{data} to \texttt{myMatrix->value} is cross-allocation block, and a deep copy automatically happens to ensure that
the current block is zero-cost copy-able and movable.  

Such cross-block assignments require deep copies and are expensive,
but in practice, such they are rare, and a programmer who understands the cost can often avoid them, making sure to allocate
data that must be kept together to the same block.
Again, this is in-keeping with PC's design philosophy: trust the ability of the programmer to do the right thing, in the small.

\subsection{The PC Object Model and Multiple Threads}

While smart-pointer-based memory management
systems are often significantly faster than garbage collected systems, such systems still have their bottlenecks.  One of the bottlenecks is concurrency
control.
Since an object can have pointers across multiple threads, smart pointer counters must be locked before increment/decrement, which
can have a significant impact on performance.  PC, however, does not need to lock reference counts (or active object counts) because only managed blocks maintain
object reference counts and active object counts,
and a block can only be managed by a single thread.  
If a thread copies a \texttt{Handle} object referencing an object housed on another thread's managed block, 
the reference count will not be changed because from the copying thread's point-of-view, the allocation block is not managed.
This can, in theory, result in a problematic case where one thread has a \texttt{Handle} to an object that has been deallocated on the other thread (since
the reference count on the home thread will not be updated to reflect
the off-thread reference).  But in practice, it tends not to be a
problem.  Parallel and distributed processing is transparent to PC application
programmers, and they typically do not write explicitly multi-threaded code, so most cross-thread references happen as the result of computations staged by the 
PC execution engine.  The PC execution engine typically uses pages carefully so as to ensure that 
it is not possible for pages to be unpinned while references to them can still exist.

%\subsection{Data Loading}
%
%PC \texttt{Handle} objects also provide the basis for moving objects to the cloud.  A programmer first creates an allocation block, then
%creates a Vector to hold all of the objects, adds zero or more objects to the Vector, and sends the Vector to the cloud: 
%
%\begin{code}
%makeObjectAllocatorBlock (1024 * 1204);
%Handle <Vector <Handle <SparseSubMatrix>>> myVec = makeObject Vector <Handle <SparseSubMatrix>> ();
%Handle <SparseSubMatrix> storeMe = makeObject <SparseSubMatrix> ();
%myVec->push_back (storeMe);
%pcContext.createSet <SparseSubMatrix> ("Mydb", "Myset");
%pcContext.sendData <SparseSubMatrix> ("Mydb", "Myset", myVec);
%\end{code}
%
%All objects loaded to the cloud are loaded via \texttt{Handle} because then the objects can be manipulated in the cloud using
%runtime polymorphism.  It is fine when writing PC code to implicitly up-cast handles; a \texttt{Handle} object pointing to an object
%of any time can always be assigned to a \texttt{Handle <Object>} (or to a \texttt{Handle} to any type that is higher in the type
%hierarchy):
%
%\begin{code}
%Handle <SparseSubMatrix> storeMe = makeObject <SparseSubMatrix> ();
%Handle <Object> meTo = storeMe;
%\end{code}
%
%One of the basic polymorphic operations that must be over-ridden by all types descending from PC's \texttt{Object} class
%is \texttt{setupAndCopyFrom (void *, void *)} (in PC's C++ binding, this is done via macro expansion).
%This operation accepts as input a pointer to a target memory location large enough to store an object of the target type;
%the target memory location is first initialized (if needed), and then a deep copy from the source memory location is performed.
%PC uses this operation during data loading.  A PC process loading data views each set of newly-stored objects as a 
%\texttt{Handle <Vector <Handle <Object>>>}; it can then move those objects around via the assignment operator on 
%the \texttt{Handle} class, which in turn calls \texttt{setupAndCopyFrom} on the underlying type.

