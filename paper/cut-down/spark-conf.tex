\section{Spark Configuration}
\label{sec:spark}

The configuration of the Spark cluster such as number of
executors, executor memory, number of cores for each executor, driver
memory and so on were carefully tuned for each
experiment, as shown in Table~\ref{fig:sparkConf}.

For TPC-H and LDA, of which total volume of data for input and
processing exceeded available memory, we ran Spark in yarn client
mode to avoid out-of-memory errors. For other experiments, we ran Spark
in cluster mode to be consistent with PC.

In addition, input data for experiments
using Dataset APIs were stored in Parquet format, and input data for
experiments using RDD APIs were stored in Spark's object file format,
and serialized using Kryo. Other Spark parameters such as parallelsim,
partition number, and so on were all carefully tuned for each experiment. More details
are omitted due to space limitation.

\begin{table}[H]
\begin{center}
\begin{tabular}{|c||c|c|c|c|}
\hline
Platform & \makecell{num\\executors} & \makecell{executor \\mem} & \makecell{executor \\cores}& \makecell{driver \\mem}\\
\hline
\texttt{lilLinAlg} &10 & 60GB & 8 & 50GB \\
TPC-H &10 & 50GB & 7 & 50GB \\
LDA &20 & 26.5GB$*$ &4 & 55GB\\
GMM&80 & 70GB & 1 & 55GB\\
$k$-means &10 &60GB & 8 & 50GB\\
\hline
\end{tabular}
\caption{Workload-specific Spark Configurations for Different
  Experiments. A star ($*$) indicates additional 4GB off heap memory is used.}
\label{fig:sparkConf}
\end{center}
\end{table}
