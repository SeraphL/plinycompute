
\section{Query Execution Examples}
\label{sec:implementation}

The \emph{distributed query scheduler} is responsible for accepting optimized TCAP program.
It dynamically transforms a TCAP program into a set of \texttt{JobStage}s,
such as:
\texttt{PipelineJobStage}, which consists of a series of pipeline
stages that run together as a pipeline of vectorized processing, as
described in Section~\ref{sec:vectorized} in the paper;
\texttt{AggregationJobStage}, to perform aggregation on shuffled data that is
generated by a \texttt{PipelineJobStage}; and \texttt{BuildHashTableJobStage}, to build hash
tables using shuffled data or broadcasted data that is generated by a
\texttt{PipelineJobStag e}. Then it
dynamically dispatches those \texttt{JobStage}s to workers as well as initiates and
monitors execution on workers. 

All computations in PC are actually performed in worker backend process. We
describe how PC implements distributed hash-partition join as an example.

\vspace{5pt}
\noindent
\textbf{Hash Partition Join}

We now briefly describe how PC implements a distributed equi-join of $n$ different sets using a hash-partition strategy.  
That is, imagine that $t$ = $\langle t_1, t_2, ..., t_n \rangle$ is a
tuple formed by taking one item from each of the $n$ sets.  Let $key(t_i)$ denote the join key of $t_i$.  Then an $n$-way equi-join requires that
in order for $t$ to be in the output set, it must be the case that
$key(t_i) = key(t_j)$ forall $i, j$. 

In PC, such an operation is broken into $2n$ \texttt{JobStage}s. The first $n$
\texttt{JobStage}s are \texttt{PipelineJobStage}s hashing and repartitioning each of the $n$ sets, so that all of the data with the same join key value
must be co-located on the same machine.  The next $n - 1$ \texttt{JobStage}s
are \texttt{BuildHashTableJobStage}s that build hash tables for all of the
entries in $n - 1$ of those sets.  The last \texttt{JobStage} is also a 
\texttt{PipelineJobStage} that scans the last set and
probes the constructed hash tables.  

In more detail, the three types of stages are:

\vspace{5pt}
{\bf 1. The data repartition stages.} This class of \texttt{JobStage}s are
\texttt{PipelineJobStage}s. It is similar to the producing stage in distributed
aggregation, with one key difference.
Rather than using a \texttt{Vector <Handle <Map <Object, Object}\texttt{>}\texttt{>}\texttt{>} to store (key, value) pairs where the value is the result obtained by aggregating a set of
data with the same key, the pipe sink used is instead a \texttt{Vector <Handle <Map <unsigned\_t, Vector <Object}\texttt{>}\texttt{>}\texttt{>}\texttt{>} data structure.  Here, the
\texttt{unsigned\_t} is a hash value produced by a TCAP \texttt{HASH} operation over the input object's join key, and the \texttt{Vector <Object>} is a list of objects
with the same hash value.  
When a new object with the same hash key is found during pipelined processing or during combining, rather than aggregating, the new object is instead inserted
into the inner \texttt{Vector <Object>} data structure that contains a set of objects with the same hash key.
Note that after the data repartition job stages completes, all of the data from all of the sets will have been repartitioned, so that all of the data with the same join
key will be co-located on the same machine.

\vspace{5pt}
{\bf 2. The hash table building stages.} These \texttt{JobStage}s are
\texttt{Buil dHashTableJobStage}s, which are similar to the consuming job stage in aggregation, 
except that again, rather than aggregating, the goal is to build up \texttt{Vector}s of objects, stored in various \texttt{Map} data structures (one for each
aggregation thread), where the \texttt{Vector} of objects associated with a particular 
\texttt{unsigned\_t} contains \emph{all} of the objects in a set whose join key hashes to that particular \texttt{unsigned\_t} value.
As a result of this class of \texttt{JobStage}s, the contents of $n - 1$ of the input sets will be stored precisely as required.

\vspace{5pt}
{\bf 3. The hash join stage.} 
This \texttt{JobStage} is also a \texttt{PipelineJo bStage}. It runs over the last set (the one that was not 
hashed as part of join stage 2), and output results to the next
\texttt{JobStage} (through shuffling) or an Output sink (e.g. write to storage
as final results). Now imagine that we are processing the last dataset, after shuffling.  At this point,
all of the other sets have been
stored in \texttt{Map <unsigned\_t, Vector <Object}\texttt{>}\texttt{>} objects.  As we process the final set,
the hash value for each object is used to probe the \texttt{Map}
associated with each of the other $n-1$ sets.  If a match is found in each one of those other $n-1$ sets, then
one or more entries in the output vector list are created to store the matches.  The resulting vector sets are then pushed through a pipeline that post-processes
the data in the vector list, likely checking to see if this was an actual match (and not just an accidental hash collision) and perhaps performing the processing
necessary to prepare for the \emph{next} join or aggregation.
Note that if $m_i$ objects from the $i$th input set have the same \texttt{unsigned\_t} hash value, then $\prod_i m_i$ entries in a vector set will be created
as a result of these hash table probes. 

%\subsection{Dynamic Query Scheduling}
%Now we briefly describe the implementation of dynamic scheduling of job stages. 
%
%A TCAP compiler compiles the TCAP automatically coded from user query graph into a query plan that is a graph connecting TCAP instructions with native code generated for executing each instruction. 
%
%Then the TCAP analyzer will search the query graph to look for the best job stage to execute, based on a cost model that considers size of source sets, size of sink set and data locality. The goal is to minimize generated intermediate data size,  and in the same time to execute all computations that are related with one dataset as close as possible to improve memory locality and avoid unnecessary data eviction from memory.
%
%%%Once the optimal job stage is identified, the distributed query scheduler will allocate datasets through the distributed storage manager, schedule the stage to run across the cluster, collect the statistics such as the size of result datasets, and remove unused datasets. Then based on the updated statistics, a new job stage will be identified as the optimal one and get scheduled until the query plan has been fully executed.
