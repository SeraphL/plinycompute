% http://www.vldb.org/pvldb/vol9/p1707-pirk.pdf
% https://cs.stanford.edu/~matei/papers/2017/cidr_weld.pdf
% http://www.vldb.org/pvldb/vol7/p853-klonatos.pdf

\section{Introduction}

Big Data systems such as Spark \cite{zaharia2010spark} and Flink \cite{alexandrov2014stratosphere, carbone2015apache}
have effectively solved what we call the ``data munging'' problem.  That is, 
these systems do an excellent job supporting the rapid
and robust development of problem-specific,
distributed/parallel codes that 
extract actionable information from the structured or unstructured data.
But while existing Big Data systems offer
excellent support for data munging,
there is a class of applications for which 
existing systems are 
used, but arguably are far less suitable:
as a platform 
for the development of high-performance codes, especially reusable
Big Data tools and libraries, by a capable systems programmer.

The desire to build new tools
on top of existing Big Data systems is understandable.  
The  developer of a distributed data processing tool must worry about data persistence, movement of
data to/from secondary storage, data
and task distribution, resource allocation, load balancing, fault tolerance, and many other factors.
While classical high-performance computing (HPC)
tools such as MPI \cite{gropp1996high} do not provide support for all of these concerns,
existing Big Data systems 
address them quite well.
As a result, many tools and libraries have been built on top of existing systems.  For example,
Spark supports
machine learning (ML) libraries Mahout \cite{mahout}, Dl4j \cite{dj4j}, 
and Spark \texttt{mllib} \cite{meng2016mllib}, linear algebra packages such as SystemML \cite{tian2012scalable, boehm2016systemml, ghoting2011systemml, boehm2014hybrid} and Samsara \cite{samsara}, and graph analytics with
GraphX \cite{gonzalez2014graphx} and GraphFrames
\cite{dave2016graphframes}.  Examples abound.

\vspace{5 pt}
\noindent
\textbf{PlinyCompute: A platform for high-performance, Big Data computing.}
However, we assert that if one were to develop a system purely for developing high-performance
Big Data codes
by a capable systems programmer,
it would not look like existing systems such as Spark and Flink,
which have largely 
been built using high-level programming languages and managed runtimes such as the JVM. 
Virtual machines abstract away
most details regarding memory management
from the system designer, including memory deallocation, reuse, and movement, as well as pointers,
object serialization and deserialization.
Since managing and utilizing memory is 
one of the most important factors determining big data system performance, reliance
on a managed environment can mean an order-of-magnitude increase in
CPU cost for some computations~\cite{blackburn2006dacapo}.  
This cost may be unacceptable 
for high-performance tool
development by an expert. 

This paper is concerned with the design and implementation of
\emph{PlinyCompute}, a system for development of
high-performance, data-intensive, distributed computing codes, especially tools and libraries.
PlinyCompute, or PC for short, is designed to fill the gap between
HPC softwares such as OpenMP \cite{dagum1998openmp} and MPI \cite{gropp1996high}, which provide little direct support for
managing very large datasets, and dataflow platforms such as Spark and Flink, which 
may give up significant performance through their reliance on a managed runtime to handle
memory management (including layout and de/allocation) and key computational considerations
such as virtual function dispatch. 

\vspace{5 pt}
\noindent
\textbf{Core design principle: Declarative in the large, high-perform-
  ance in the small.}
PC is unique in that \emph{in the large}, 
it presents the programmer with a very high-level,
declarative interface, relying on automatic, 
relational-database style optimization \cite{chaudhuri1998overview} to figure out how to stage
distributed computations.  
PC's declarative interface is higher-level than competing systems, in that decisions such as choice of join ordering and which
join algorithms to run are
totally under control of the system. 
This is particularly important for tool and library development because the same tool should run well regardless of the data
it is applied to---the classical idea of \emph{data independence} in database system design \cite{stonebraker1990third}.
A relatively naive library user cannot be expected to tune a library implementation of an algorithm to run
well on his or her particular dataset, and yet with existing systems, this sort of tuning
may require modification of the
code itself, which is beyond the vast majority of end-users.

In contrast, \emph{in the small}, PlinyCompute presents a capable programmer with a
persistent object data model and API (the ``PC object model'') and associated memory management system
designed from the ground-up for
high performance.
All data processed by PC are managed by
the PC object
model, which is exclusively responsible for PC data layout and within-page memory management.  
The PC object model is tightly coupled with
PC's execution engine, and has been specifically designed for efficient distributed computing.  
All dynamic PC \texttt{Object} allocation is \emph{in-place}, directly on a page, obviating
the need for PC \texttt{Object} serialization and deserialization
before data are transferred to/from storage or over a network.
Further, PC gives a programmer fine-grained control of the system
memory management and PC \texttt{Object} re-use policies.

This hybrid approach---declarative and yet trusting the programmer
to utilize PC object model effectively
in the small---results in a system ideal for the 
development of data-oriented tools and libraries.

The system consists of four main components: 

\vspace{3pt}
\noindent
(1) The \emph{PC object model}, which is a toolkit for building high-performance, persistent data structures.

\vspace {3pt}
\noindent
(2) The \emph{PC API and TCAP compiler}.  In the large, PC codes are declarative and look a lot like classical relational calculus \cite{codd1971data}.  For example, to specify a join over five sets of objects, a PC programmer does not build a join DAG over the five inputs, as in a standard
dataflow platform such as Spark.  Rather, a programmer 
supplies two \emph{lambda term construction functions}: one that constructs a lambda term describing the selection
predicate over those five input sets, 
and a second that constructs a lambda term describing the relational projection over those five sets
using the same API.  These lambda terms are constructed using PC's built-in lambda abstraction families as well as higher-order composition functions.
 PC's TCAP compiler 
accepts such a specification, and compiles it into a functional, domain specific language called \emph{TCAP} that implements
the join.  Logically, TCAP operates over
sets of columns of PC \texttt{Object}s. 

\vspace{3pt}
\noindent
(3) The \emph{execution engine}, which is a distributed query processing
  system for Big Data analytics.
  It consists of an optimizer for TCAP
  programs and a 
high-performance, distributed, vectorized TCAP processing engine. 
The TCAP processing engine 
and has been designed to work closely with the PC object model to
minimize memory-related costs during computation.

\vspace{3pt}
\noindent
(4) Various \emph{distributed services}, which include 
a catalog manager serving system meta-data, and a
distributed storage manager.


%\end{itemize}

\vspace{8 pt}
\noindent
\textbf{Our contributions.}
We describe the design and implementation of PlinyCompute.  
Currently, PC consists of around
150,000 lines of C++ code, with a small amount of Prolog code,
We benchmark several library-style softwares written on top of PC.  We begin with a small domain specific language
for distributed linear algebra that we implemented on top of PC,
called \texttt{lilLinAlg}.  \texttt{lilLinAlg} was implemented in
about six weeks by a capable developer
who had no knowledge of PC at the outset, with the goal of demonstrating PC's suitability as a tool-development platform.  
We also benchmark the efficiency of manipulating complex objects, and also several standard machine learning codes written on top of PC.
