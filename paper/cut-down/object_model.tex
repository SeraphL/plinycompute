\section{The Object Model in Detail} \label{sec:ObjectModel}

%The PC object model grants programmers fine-grained control over how data are managed in the small, 
%and 
%yet still provides a great deal of
%key functionalities.

\subsection{Using PC Objects}
\label{sec:pc-object1}
Arguably, the choice of how individual data items are to be represented and manipulated
in a data analytics or management system is one of the most
%consequential%
controversial decisions
that a system designer can make, both in terms of 
the programmability of the resulting system, and its performance.  For decades, the dominant model used in
data management was the flat relational model, which 
can achieve very good performance.
Flatness generally means 
that there is typically no distinction between the in-memory representation of data, and the on-disk (or in-network) representation of
data. Thus there is no (de-)serialization cost to move data to/from
disk and network, and memory management costs are very low. 

The downside is that flat relations are very limiting to a programmer.  Modern, object-based 
data analytics systems 
(such as Spark via its Resillient Distributed Dataset (RDD) interface \cite{zaharia2012resilient}) offer far more flexibility, at the (possible) cost of significant performance degradation.  
PC attempts to combine this flexibility with excellent performance.
The PC object model provides a fully object-oriented interface, supporting the standard functionality expected in a modern, object-oriented type system,
including generic programming (the PC object model supports generic \texttt{Map} and \texttt{Vector} types), 
pointers (or, more specifically,
``pointer-like'' objects called PC \texttt{Handle} objects), inheritance, and dynamic dispatch for runtime polymorphism.  

For example, imagine that the goal is implementing a distributed linear algebra system on top of the PC object model, where huge matrices are ``chunked'' into
smaller sub-matrices.  A sub-matrix may be stored via our current, C++ binding, using the following object:

\begin{codesmall}
class MatrixBlock : public Object {
public:
	int chunkRow, chunkColumn;
	int chunkWidth, chunkHeight;
	Vector <double> values; 
};
\end{codesmall}

or, a sparse sub-matrix may be stored as:

\begin{codesmall}
class SparseMatrixBlock : public Object {
public:
	int chunkRow, chunkColumn;
	int chunkWidth, chunkHeight;
	Map <pair <int, int>, double> values; 
};
\end{codesmall}

In the sparse sub-matrix, the \texttt{pair <int, int>} indexes a non-zero entry in the chunk by its row and column.

But while the PC object model provides a rich, object-oriented programming model, it also provides the good performance characteristic
of a flat relational model.
The key principle underlying the PC object model is \emph{zero-cost data movement}.  That is, once a data object
has been allocated and populated, moving the object to disk or across
the network should be a simple matter of copying memory; there
should be no CPU cost for serialization and deserialization.

At first glance, it would seem to be impossible to offer zero-cost data movement while allowing a programmer to create and manipulate such objects.  
Pointers and container classes 
generally lead to high memory (de)allocation costs and high object (de)serialization costs, resulting in high CPU cost.
The PC object model avoids this 
by using a ``page-as-a-heap'' memory allocation model.  
The PC object model provides a call of the form:

\begin{codesmall}
makeObjectAllocationBlock (ptr, blockSize);
\end{codesmall}

After such a call, \emph{all subsequent PC Object allocations by the thread creating the object allocation block will be performed directly to the memory
region starting at location} \texttt{ptr}.
Typically, when it runs a computation, PC's execution engine will obtain a page from its memory pool to buffer output data, calling
PC's
\texttt{makeObjectAllocationBlock ()} function with a pointer to the page where output data are to be written.  
When an action taken by the execution engine or user-supplied code causes an
out-of-memory execution, it means that the page is full.  At that point, the execution engine can take appropriate computation-specific 
action, such as creating 
an object allocation block out of a new (empty) page, writing the full page out to disk, sending it across the network, etc.  
No serialization or deserialization or any sort of post-processing are needed, 
because all object allocations have taken place exclusively to the current allocation block.  

In order to guarantee zero-cost data movement,
one rule that a PC programmer must follow is that any object that will
be loaded into the distributed PC runtime must either be of a ``simple'' type (a simple type
must contain no raw C-style pointers and no 
virtual functions, and a \texttt{memmove}
must suffice to copy the object), or else it must descend from PC's \texttt{Object} class, which serves as the base for all complex object types.
Complex objects are those that include containers (\texttt{Vector}, \texttt{Map}) or pointer-like \texttt{Handle} objects.  Descending from PC's \texttt{Object} class
ensures that the resulting class type has a set of virtual functions
that allow it to be manipulated in and transferred across the
distributed PC runtime, 
such as a virtual deep copy function.  

\subsection{PC Handles}

To support linked data structures, dynamic allocation, and runtime polymorphism, it is necessary for a 
system to provide pointer-like functionality.  This is provided by PC's built-in \texttt{Handle} type.  
A \texttt{Handle} to an object is returned from a dynamic allocation to the current allocation block.
Such as the following statement:

\begin{codesmall}
Handle<MatrixBlock>mySubMatrix=makeObject<MatrixBlock>();
\end{codesmall}

Internally, PC \texttt{Handle} objects contain two pieces of data: an \emph{offset pointer} that tells how far
the physical address of the object being pointed to is from the physical location of the 
\texttt{Handle}, and a \emph{type code} that stores the type of the
object that is pointed to.  

PC uses an offset pointer rather than a classical, C-style pointer in order to support
zero-cost data movement.  
A \texttt{Handle} may begin its life allocated
to one page, which may be stored on disk, then sent across a network
to another process.  The \texttt{Handle} pointer
can function correctly at the new process.  
An actual C-style pointer
cannot survive translation from one process to another, as the program will be mapped to a different location in memory.

\subsection{Dynamic Dispatch}
\label{sec:dyn_dis}

In PC, dynamic dispatch for virtual function calls is facilitated by the type code stored within each
\texttt{Handle} object.
Each type code begins with a bit that denotes whether or not the referenced type is a simple type (which, by definition, cannot have any
virtual functions and for which a \texttt{memmove} suffices to perform a copy) or a type descended from PC's \texttt{Object} base class.

In the case of a PC \texttt{Object} or its descendants, the
type code is a unique identifier for the PC-\texttt{Object}-descended type of the object that the \texttt{Handle} points to.
In every major C++ compiler (GCC, clang, Intel, and Microsoft), virtual functions
are implemented using a virtual function table, or \texttt{vTable} object, a pointer to which is located at the beginning
of each C++ object having a virtual function.  Unfortunately, the
\texttt{vTable} pointer is a native, C-style pointer, and it does not automatically translate when an
object is moved across processes.  To handle this, in
PC's C++ binding, whenever a \texttt{Handle} object is dereferenced, 
a lookup using the
type code retrieves a process-specific pointer to that class' \texttt{vTable} object.

Obtaining a pointer to a class' \texttt{vTable} object is not straightforward.
A user may run code on his/her machine that creates
a PC \texttt{Object}, and then ships that PC \texttt{Object} into the distributed
PC runtime.  At the other end, it arrives at a PC worker process that has never seen that type of object before and hence
does not have access to a \texttt{vTable} pointer for that class.
PC addresses this issue by requiring that all classes deriving from PC's \texttt{Object} base class be registered with the catalog
manager before they are loaded into the distributed storage system.  This registration requires shipping a library file (a \texttt{.so} file in Linux/Unix) to
the catalog manager.  This library exposes a special \texttt{getVTablePtr ()} function that returns a pointer to the \texttt{vTable} for the class contained
in the \texttt{.so} file.

A \texttt{vTable} pointer lookup first goes to the PC process' \texttt{vTable} lookup table.  When this 
lookup fails (because the process has not yet seen a \texttt{vTable} pointer for that class
type) the request then goes to the catalog manager, which responds to the process with a copy of the appropriate \texttt{.so} file.  This 
\texttt{.so} file is then 
dynamically loaded into the process' address space, the \texttt{vTable} pointer is located in the library,
and returned to the requester.

In this way, PC provides something akin to the automated,
dynamic loading of classes (via JVM \texttt{.class} files) that is
provided by most big data systems.  
Objects of arbitrary type can be loaded into the distributed PC runtime and be
processed using dynamically loaded native code, as long
as the object type is registered first.

\subsection{Allocation, Deallocation, and Assignment}

There are three types of allocation blocks in PC, where an ``allocation block'' is a block of memory where PC \texttt{Object}s can be
allocated.

\vspace{5pt}
\noindent
\textbf{Active allocation block.} Each thread running in a PC process has exactly one \emph{active} allocation block, that is currently receiving allocations (all calls to
\texttt{makeObject} cause memory allocations to happen using that block).  Such an allocation block is created via a call to 
\texttt{makeObjectAllocationBlock ()}.  User code typically creates and manipulates objects in this block.

\vspace{5pt}
\noindent
\textbf{Inactive allocation block.} Each thread also has one or more
\emph{inactive}, \emph{managed} blocks.  These are previously-active blocks of memory that contain one or more objects that are reachable
from some \texttt{Handle} that is currently in RAM.  When the number
of reachable objects in such a block drops to zero, it is automatically
deallocated.
When a user (or the PC system software) calls 
\texttt{makeObjectAlloc ationBlock ()}, the newly created allocation block becomes the active block, and if the previously-active allocation block has any
reachable objects on it, it becomes an inactive, managed block.

\vspace{5pt}
\noindent
\textbf{Unmanaged allocation block.} Finally, there are zero or more \emph{inactive},
  \emph{un-managed} blocks.  These are blocks with reachable PC
  \texttt{Object}s that are \emph{not} managed by the PC object model.  These tend to be pages of objects that have
been loaded into RAM from disk or across the network for processing
during a distributed computation.  Such blocks are paged in and out of the
buffer pool in much the same way as a relational database would page data in and out.
Rather than the PC object model being responsible for managing such blocks, PC's execution engine manages such blocks.

\vspace{5pt}

In PC, each managed allocation block (active or inactive) has an active object counter (the number of objects that are reachable
from some \texttt{Handle} in RAM).  Each object in each managed allocation block (active or inactive) is reference counted, or pre-pended with a count of
the number of \texttt{Handle} objects that currently reference the object.  
Un-managed blocks (and objects inside of such blocks) are not reference-counted.

When the reference count on an object in a managed block goes to zero, it is automatically
deallocated (at least, this is the default behavior; it is possible
for a programmer to override this behavior for speed, if desired, as
we describe in Section~\ref{sec:allocation}).  
Once the number of reachable objects on an inactive, managed allocation block falls to zero, the block is automatically deallocated.  
Since the fundamental goal of PC object model design is 
zero-cost data movement---an allocation block should be
transferable across processes and machines immediately usable with no pre- or post-processing---one
potential problem is dangling \texttt{Handle}s.  Specifically: What happens when there is a \texttt{Handle} located in one allocation block that points to a PC
\texttt{Object} located in another allocation block?  The \texttt{Handle} may be valid, but when the \texttt{Handle}'s allocation block is moved to a new process where
the target block is not located, the\texttt{Handle} cannot be dereferenced without a runtime error. 
PC simply prevents this situation from ever happening. Whenever an assignment operation on \texttt{Handle} that is physically located
in the active allocation block results in that
\texttt{Handle} pointing outside of the block, a deep copy of the target of the assignment
is automatically performed.  This deep copy happens recursively, so any \texttt{Handle}s in the copied object that point outside of the active allocation block
have \emph{their} targets deep copied to the active block.  For example, consider the following code:

\begin{codesmall}
makeObjectAllocatorBlock (1024 * 1204);
Handle <Vector <double>> data = 
     makeObject <Vector <double>> ();
for (int i = 0; i < 1000; i++)
     data->push_back (i * 1.0);
makeObjectAllocatorBlock (1024 * 1204); 
Handle <MatrixBlock> myMatrix = 
     makeObject <MatrixBlock> ();
myMatrix->value = data; // deep copy of data happens!!
\end{codesmall}

At the second \texttt{makeObjectAllocatorBlock}, the original allocation block, holding the list of \texttt{double}s pointed to by \texttt{data}, becomes
inactive.  The submatrix \texttt{myMatrix} is allocated to the new active block.  
Hence, the assignment of \texttt{data} to \texttt{myMatrix->value} is cross-allocation block, and a deep copy automatically happens to ensure that
the current block is zero-cost copy-able.  

Such cross-block assignments require deep copies and are
expensive, but they are rare, and a programmer who understands the cost can often avoid them, making sure to allocate
data that must be kept together to the same block.
Again, this is in-keeping with PC's design philosophy: trust the ability of the programmer to do the right thing, in the small.

\subsection{The PC Object Model and Multiple Threads}

PC does not need to lock reference counts during multi-threaded execution
because 
a block can only be managed by one single thread.  
If a thread copies a \texttt{Handle} object referencing an object housed on another thread's managed block, 
the reference count will not be changed because from the copying thread's point-of-view, the allocation block is not managed.
This can, in theory, result in a problematic case where one thread has a \texttt{Handle} to an object that has been deallocated on the other thread (since
the reference count on the home thread will not be updated to reflect
the off-thread reference).  But in practice, this is not
problem.  Parallel and distributed processing is transparent to PC application
programmers, so most cross-thread references happen as the result of computations staged by the 
PC execution engine.  The PC execution engine uses pages carefully so as to ensure that 
it is not possible for pages to be unpinned while references to them can still exist.

%\subsection{Data Loading}
%
%PC \texttt{Handle} objects also provide the basis for moving objects to the cloud.  A programmer first creates an allocation block, then
%creates a Vector to hold all of the objects, adds zero or more objects to the Vector, and sends the Vector to the cloud: 
%
%\begin{code}
%makeObjectAllocatorBlock (1024 * 1204);
%Handle <Vector <Handle <SparseSubMatrix>>> myVec = makeObject Vector <Handle <SparseSubMatrix>> ();
%Handle <SparseSubMatrix> storeMe = makeObject <SparseSubMatrix> ();
%myVec->push_back (storeMe);
%pcContext.createSet <SparseSubMatrix> ("Mydb", "Myset");
%pcContext.sendData <SparseSubMatrix> ("Mydb", "Myset", myVec);
%\end{code}
%
%All objects loaded to the cloud are loaded via \texttt{Handle} because then the objects can be manipulated in the cloud using
%runtime polymorphism.  It is fine when writing PC code to implicitly up-cast handles; a \texttt{Handle} object pointing to an object
%of any time can always be assigned to a \texttt{Handle <Object>} (or to a \texttt{Handle} to any type that is higher in the type
%hierarchy):
%
%\begin{code}
%Handle <SparseSubMatrix> storeMe = makeObject <SparseSubMatrix> ();
%Handle <Object> meTo = storeMe;
%\end{code}
%
%One of the basic polymorphic operations that must be over-ridden by all types descending from PC's \texttt{Object} class
%is \texttt{setupAndCopyFrom (void *, void *)} (in PC's C++ binding, this is done via macro expansion).
%This operation accepts as input a pointer to a target memory location large enough to store an object of the target type;
%the target memory location is first initialized (if needed), and then a deep copy from the source memory location is performed.
%PC uses this operation during data loading.  A PC process loading data views each set of newly-stored objects as a 
%\texttt{Handle <Vector <Handle <Object>>>}; it can then move those objects around via the assignment operator on 
%the \texttt{Handle} class, which in turn calls \texttt{setupAndCopyFrom} on the underlying type.

